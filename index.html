<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Analyzer & AI Mimic</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@300;400;600;800&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Plus Jakarta Sans', sans-serif; background-color: #0f172a; color: #f8fafc; }
        
        /* Mic Pulse */
        @keyframes pulse-ring {
            0% { transform: scale(0.8); box-shadow: 0 0 0 0 rgba(239, 68, 68, 0.7); }
            70% { transform: scale(1); box-shadow: 0 0 0 15px rgba(239, 68, 68, 0); }
            100% { transform: scale(0.8); box-shadow: 0 0 0 0 rgba(239, 68, 68, 0); }
        }
        .recording-btn {
            background-color: #ef4444 !important;
            animation: pulse-ring 1.5s infinite;
        }

        /* Glass Cards */
        .glass { background: rgba(30, 41, 59, 0.7); backdrop-filter: blur(10px); border: 1px solid rgba(255, 255, 255, 0.05); }
        
        /* Audio visualizer glow */
        canvas { filter: drop-shadow(0 0 5px rgba(56, 189, 248, 0.5)); }
    </style>
</head>
<body class="min-h-screen p-4 flex flex-col items-center py-10 bg-[radial-gradient(ellipse_at_top,_var(--tw-gradient-stops))] from-slate-800 via-slate-900 to-black">

    <div class="max-w-3xl w-full space-y-6">
        
        <!-- Header -->
        <div class="text-center space-y-2 mb-8">
            <h1 class="text-3xl font-extrabold text-transparent bg-clip-text bg-gradient-to-r from-cyan-400 to-blue-500">
                <i class="fas fa-headset text-cyan-400"></i> Voice Clone & Mimic
            </h1>
            <p class="text-slate-400">Analyze your voice profile and hear the AI replicate your pitch and pace.</p>
        </div>

        <!-- STEP 1: Input Area -->
        <div class="glass rounded-3xl p-8 relative overflow-hidden">
            <div class="absolute top-0 left-0 w-full h-1 bg-gradient-to-r from-cyan-500 to-blue-600"></div>
            
            <div class="flex flex-col md:flex-row gap-8 items-center">
                <!-- Text to Read -->
                <div class="flex-1 text-center md:text-left">
                    <span class="text-xs font-bold text-cyan-500 uppercase tracking-widest">Step 1: Read Aloud</span>
                    <p class="text-xl text-slate-200 mt-3 italic">
                        "Artificial intelligence is learning to speak, but human emotion and tone are what give words their true meaning."
                    </p>
                </div>
                
                <!-- Mic Button -->
                <div class="flex flex-col items-center justify-center shrink-0">
                    <button id="mic-btn" onclick="toggleRecording()" class="w-20 h-20 bg-cyan-600 rounded-full text-white text-3xl transition-all duration-300 hover:bg-cyan-500 shadow-lg shadow-cyan-900/50 flex items-center justify-center">
                        <i id="mic-icon" class="fas fa-microphone"></i>
                    </button>
                    <span id="mic-status" class="mt-3 text-sm font-medium text-slate-400">Click to Record</span>
                </div>
            </div>

            <!-- Visualizer -->
            <div class="w-full h-16 bg-black/50 rounded-xl mt-6 overflow-hidden relative border border-slate-700">
                <canvas id="visualizer" class="w-full h-full"></canvas>
            </div>
        </div>

        <!-- STEP 2: Analysis Results (Hidden initially) -->
        <div id="results-panel" class="grid grid-cols-2 md:grid-cols-4 gap-4 hidden">
            <!-- Avg Pitch -->
            <div class="glass rounded-2xl p-5 text-center">
                <i class="fas fa-wave-square text-blue-400 text-xl mb-2"></i>
                <div class="text-xs text-slate-400 uppercase tracking-wider font-bold mb-1">Depth (Pitch)</div>
                <div class="text-2xl font-bold text-white"><span id="res-pitch">--</span> <span class="text-sm text-slate-500">Hz</span></div>
            </div>
            
            <!-- Range -->
            <div class="glass rounded-2xl p-5 text-center">
                <i class="fas fa-arrows-up-down text-purple-400 text-xl mb-2"></i>
                <div class="text-xs text-slate-400 uppercase tracking-wider font-bold mb-1">Freq Range</div>
                <div class="text-lg font-bold text-white"><span id="res-range">--</span></div>
            </div>

            <!-- Tone Variance -->
            <div class="glass rounded-2xl p-5 text-center">
                <i class="fas fa-music text-pink-400 text-xl mb-2"></i>
                <div class="text-xs text-slate-400 uppercase tracking-wider font-bold mb-1">Tone</div>
                <div class="text-lg font-bold text-white" id="res-tone">--</div>
            </div>

            <!-- Pace -->
            <div class="glass rounded-2xl p-5 text-center">
                <i class="fas fa-stopwatch text-emerald-400 text-xl mb-2"></i>
                <div class="text-xs text-slate-400 uppercase tracking-wider font-bold mb-1">Pace</div>
                <div class="text-2xl font-bold text-white"><span id="res-pace">--</span> <span class="text-sm text-slate-500">SPM</span></div>
            </div>
        </div>

        <!-- STEP 3: The Mimic Engine (Hidden initially) -->
        <div id="mimic-panel" class="glass rounded-3xl p-8 relative overflow-hidden hidden">
            <div class="absolute top-0 left-0 w-full h-1 bg-gradient-to-r from-purple-500 to-pink-600"></div>
            
            <h2 class="text-xl font-bold text-white mb-2"><i class="fas fa-robot text-purple-400 mr-2"></i>Step 3: AI Voice Mimic</h2>
            <p class="text-sm text-slate-400 mb-6">The AI has loaded your pitch and pace profile. Type something below and hear it speak using your parameters.</p>

            <div class="flex flex-col md:flex-row gap-4">
                <input type="text" id="mimic-text" value="Hello! I am a synthetic voice adjusted to match your pitch and speed." 
                    class="flex-1 bg-slate-800 border border-slate-600 text-white rounded-xl px-4 py-3 focus:outline-none focus:border-purple-500 focus:ring-1 focus:ring-purple-500 transition-all">
                
                <button onclick="mimicVoice()" class="bg-gradient-to-r from-purple-600 to-pink-600 hover:from-purple-500 hover:to-pink-500 text-white font-bold py-3 px-8 rounded-xl flex items-center justify-center gap-2 shadow-lg transition-transform transform hover:scale-105">
                    <i class="fas fa-play"></i> Speak
                </button>
            </div>
            
            <div class="mt-4 text-xs text-slate-500 flex justify-between bg-black/30 p-3 rounded-lg border border-slate-700">
                <span>Applied AI Pitch: <strong id="debug-pitch" class="text-purple-400">1.0</strong></span>
                <span>Applied AI Speed: <strong id="debug-rate" class="text-emerald-400">1.0</strong></span>
            </div>
        </div>

    </div>

    <script>
        // --- State ---
        let audioCtx, analyser, microphone;
        let isRecording = false;
        let animationId;
        
        // Data Arrays
        let pitchSamples = [];
        let startTime = 0;
        let syllableCount = 0;
        let isSpeaking = false;

        // User Profile Data (saved after analysis)
        let userProfile = {
            avgPitch: 150,
            paceSpm: 150
        };

        // --- UI Elements ---
        const micBtn = document.getElementById('mic-btn');
        const micIcon = document.getElementById('mic-icon');
        const micStatus = document.getElementById('mic-status');
        const canvas = document.getElementById('visualizer');
        const canvasCtx = canvas.getContext('2d');
        
        // Panels
        const resultsPanel = document.getElementById('results-panel');
        const mimicPanel = document.getElementById('mimic-panel');

        // --- Web Speech API (TTS Setup) ---
        const synth = window.speechSynthesis;
        let availableVoices = [];
        
        // Browsers load voices asynchronously
        function loadVoices() {
            availableVoices = synth.getVoices();
        }
        if (speechSynthesis.onvoiceschanged !== undefined) {
            speechSynthesis.onvoiceschanged = loadVoices;
        }

        // --- Step 1: Record & Analyze ---
        async function toggleRecording() {
            if (isRecording) {
                stopRecording();
            } else {
                startRecording();
            }
        }

        async function startRecording() {
            // Reset state
            pitchSamples = [];
            syllableCount = 0;
            startTime = Date.now();
            resultsPanel.classList.add('hidden');
            mimicPanel.classList.add('hidden');

            try {
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 2048;

                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                microphone = audioCtx.createMediaStreamSource(stream);
                microphone.connect(analyser);

                isRecording = true;
                
                // Update UI
                micBtn.classList.add('recording-btn');
                micIcon.classList.replace('fa-microphone', 'fa-stop');
                micStatus.innerText = "Recording... Click to Stop";
                micStatus.classList.add('text-red-400');

                processAudio();

            } catch (err) {
                alert("Please allow microphone access to analyze your voice.");
                console.error(err);
            }
        }

        function stopRecording() {
            if (!isRecording) return;
            isRecording = false;
            
            // Clean up Audio
            cancelAnimationFrame(animationId);
            if (microphone) microphone.disconnect();
            if (audioCtx) audioCtx.close();

            // Update UI
            micBtn.classList.remove('recording-btn');
            micIcon.classList.replace('fa-stop', 'fa-microphone');
            micStatus.innerText = "Processing Data...";
            micStatus.classList.remove('text-red-400');

            setTimeout(analyzeData, 500); // Small delay for UX
        }

        // --- Audio Processing Loop ---
        function processAudio() {
            if (!isRecording) return;
            animationId = requestAnimationFrame(processAudio);

            const bufferLength = analyser.frequencyBinCount;
            const floatData = new Float32Array(analyser.fftSize);
            const byteData = new Uint8Array(bufferLength);
            
            analyser.getFloatTimeDomainData(floatData);
            analyser.getByteTimeDomainData(byteData);

            // 1. Calculate Pitch via Autocorrelation
            const pitch = autoCorrelate(floatData, audioCtx.sampleRate);
            if (pitch !== -1 && pitch > 60 && pitch < 500) {
                pitchSamples.push(pitch);
            }

            // 2. Volume/Envelope for Syllable Detection (Pace)
            let rms = 0;
            for(let i=0; i < floatData.length; i++) rms += floatData[i] * floatData[i];
            rms = Math.sqrt(rms / floatData.length);
            
            if (rms > 0.05 && !isSpeaking) {
                isSpeaking = true;
                syllableCount++;
            } else if (rms < 0.02 && isSpeaking) {
                isSpeaking = false;
            }

            // Draw Visualizer
            drawVisualizer(byteData, bufferLength);
        }

        function drawVisualizer(dataArray, bufferLength) {
            canvasCtx.clearRect(0, 0, canvas.width, canvas.height);
            canvasCtx.lineWidth = 2;
            canvasCtx.strokeStyle = isRecording ? '#38bdf8' : '#334155';
            canvasCtx.beginPath();

            const sliceWidth = canvas.width * 1.0 / bufferLength;
            let x = 0;

            for (let i = 0; i < bufferLength; i++) {
                const v = dataArray[i] / 128.0;
                const y = v * canvas.height / 2;
                if (i === 0) canvasCtx.moveTo(x, y);
                else canvasCtx.lineTo(x, y);
                x += sliceWidth;
            }
            canvasCtx.stroke();
        }

        // --- Step 2: Math & Results ---
        function analyzeData() {
            micStatus.innerText = "Click to Record Again";
            
            if (pitchSamples.length < 5) {
                alert("Audio was too quiet or too short. Please speak normally into the microphone.");
                return;
            }

            // Calculate Pitch Metrics
            const minPitch = Math.round(Math.min(...pitchSamples));
            const maxPitch = Math.round(Math.max(...pitchSamples));
            const avgPitch = Math.round(pitchSamples.reduce((a, b) => a + b, 0) / pitchSamples.length);
            
            // Calculate Tone Variance (Standard Deviation)
            const squareDiffs = pitchSamples.map(p => Math.pow(p - avgPitch, 2));
            const variance = Math.sqrt(squareDiffs.reduce((a,b)=>a+b,0) / squareDiffs.length);
            let toneLabel = "Stable";
            if (variance < 10) toneLabel = "Monotone";
            if (variance > 30) toneLabel = "Expressive";

            // Calculate Pace (Syllables Per Minute)
            const durationSec = (Date.now() - startTime) / 1000;
            const spm = Math.round((syllableCount / durationSec) * 60);

            // Save to Global Profile for the Mimic Engine
            userProfile.avgPitch = avgPitch;
            userProfile.paceSpm = spm;

            // Update DOM
            document.getElementById('res-pitch').innerText = avgPitch;
            document.getElementById('res-range').innerText = `${minPitch} - ${maxPitch} Hz`;
            document.getElementById('res-tone').innerText = toneLabel;
            document.getElementById('res-pace').innerText = spm;

            // Show Panels
            resultsPanel.classList.remove('hidden');
            mimicPanel.classList.remove('hidden');
        }

        // --- Step 3: The Mimic Engine (TTS) ---
        function mimicVoice() {
            // Stop any current speech
            synth.cancel();

            const textToSpeak = document.getElementById('mimic-text').value;
            if (!textToSpeak) return;

            const utterance = new SpeechSynthesisUtterance(textToSpeak);

            // MAGIC PART: Mapping User Stats to AI Parameters
            
            /* PITCH MAPPING:
               Browser TTS pitch goes from 0.1 to 2.0 (1.0 is default).
               Assume 150 Hz is a standard "1.0" default browser pitch.
               Formula: (UserPitch / 150).
            */
            let aiPitch = userProfile.avgPitch / 150;
            // Clamp between 0.3 (very deep) and 1.8 (very high) to prevent robotic distortion
            aiPitch = Math.max(0.3, Math.min(1.8, aiPitch)); 
            
            /*
               RATE MAPPING:
               Browser TTS rate goes from 0.1 to 10 (1.0 is default).
               Assume 150 Syllables Per Minute is standard rate.
            */
            let aiRate = userProfile.paceSpm / 150;
            // Clamp between 0.6 (slow) and 1.5 (fast)
            aiRate = Math.max(0.6, Math.min(1.5, aiRate));

            utterance.pitch = aiPitch;
            utterance.rate = aiRate;

            // Attempt to pick matching gender voice
            if (availableVoices.length > 0) {
                let selectedVoice = null;
                // If pitch is below 160Hz, look for a male voice (often named David, Guy, Male)
                if (userProfile.avgPitch < 160) {
                    selectedVoice = availableVoices.find(v => 
                        v.name.toLowerCase().includes('male') || 
                        v.name.toLowerCase().includes('david') || 
                        v.name.toLowerCase().includes('guy')
                    );
                } else {
                    // Look for female voice
                    selectedVoice = availableVoices.find(v => 
                        v.name.toLowerCase().includes('female') || 
                        v.name.toLowerCase().includes('zira') || 
                        v.name.toLowerCase().includes('girl')
                    );
                }
                
                // Fallback to default if matching gender not found
                if (selectedVoice) {
                    utterance.voice = selectedVoice;
                }
            }

            // Update debug UI
            document.getElementById('debug-pitch').innerText = aiPitch.toFixed(2);
            document.getElementById('debug-rate').innerText = aiRate.toFixed(2);

            // Speak!
            synth.speak(utterance);
        }

        // --- Pitch Detection Algorithm ---
        function autoCorrelate(buf, sampleRate) {
            let size = buf.length;
            let rms = 0;
            for (let i = 0; i < size; i++) rms += buf[i] * buf[i];
            rms = Math.sqrt(rms / size);
            if (rms < 0.01) return -1; // Ignore silence

            let r1 = 0, r2 = size - 1, thres = 0.2;
            for (let i = 0; i < size / 2; i++) if (Math.abs(buf[i]) < thres) { r1 = i; break; }
            for (let i = 1; i < size / 2; i++) if (Math.abs(buf[size - i]) < thres) { r2 = size - i; break; }
            
            const newBuf = buf.slice(r1, r2);
            size = newBuf.length;

            const c = new Array(size).fill(0);
            for (let i = 0; i < size; i++) {
                for (let j = 0; j < size - i; j++) {
                    c[i] = c[i] + newBuf[j] * newBuf[j + i];
                }
            }
            
            let d = 0; while (c[d] > c[d + 1]) d++;
            let maxval = -1, maxpos = -1;
            for (let i = d; i < size; i++) {
                if (c[i] > maxval) {
                    maxval = c[i];
                    maxpos = i;
                }
            }
            let T0 = maxpos;
            return sampleRate / T0;
        }

        // Initialize voices on load if supported
        loadVoices();
    </script>
</body>
</html>

